----------------------------------------ARGUMENTS----------------------------------------
data_dir                                 ./data/beauty/
model_version                            0
task_num                                 3
prompt_num                               10
lr                                       0.0005
epochs                                   100
batch_size                               64
cuda                                     True
log_interval                             200
checkpoint                               ./checkpoint/beauty/
endure_times                             5
exp_len                                  20
negative_num                             99
----------------------------------------ARGUMENTS----------------------------------------
[2023-05-17 22:09:31.449951]: Loading data
[2023-05-17 22:10:28.918731]: Start training
[2023-05-17 22:10:28.918963]: epoch 1
[2023-05-17 22:11:34.926562]: text loss 2.6970 |   200/ 2358 batches
[2023-05-17 22:12:40.633515]: text loss 2.2897 |   400/ 2358 batches
[2023-05-17 22:13:44.792582]: text loss 2.2331 |   600/ 2358 batches
[2023-05-17 22:14:49.229261]: text loss 2.2118 |   800/ 2358 batches
[2023-05-17 22:15:54.678924]: text loss 2.1744 |  1000/ 2358 batches
[2023-05-17 22:16:59.960006]: text loss 2.1522 |  1200/ 2358 batches
[2023-05-17 22:18:04.804332]: text loss 2.1583 |  1400/ 2358 batches
[2023-05-17 22:19:10.860857]: text loss 2.1242 |  1600/ 2358 batches
[2023-05-17 22:20:15.968161]: text loss 2.1249 |  1800/ 2358 batches
[2023-05-17 22:21:20.298906]: text loss 2.1150 |  2000/ 2358 batches
[2023-05-17 22:22:25.542052]: text loss 2.1148 |  2200/ 2358 batches
[2023-05-17 22:23:16.441622]: text loss 2.1065 |  2358/ 2358 batches
[2023-05-17 22:23:16.442962]: validation
[2023-05-17 22:23:21.368888]: explanation loss 2.2865
[2023-05-17 22:23:41.819172]: sequential loss 2.3373
[2023-05-17 22:25:37.107889]: top-N loss 1.5221
[2023-05-17 22:25:37.108166]: total loss 2.0486
[2023-05-17 22:25:37.909851]: epoch 2
[2023-05-17 22:25:51.157292]: text loss 2.0936 |  2400/ 2358 batches
[2023-05-17 22:26:56.083619]: text loss 2.1037 |  2600/ 2358 batches
[2023-05-17 22:28:00.487963]: text loss 2.0938 |  2800/ 2358 batches
[2023-05-17 22:29:05.589227]: text loss 2.0917 |  3000/ 2358 batches
[2023-05-17 22:30:10.450097]: text loss 2.0777 |  3200/ 2358 batches
[2023-05-17 22:31:14.694079]: text loss 2.0847 |  3400/ 2358 batches
[2023-05-17 22:32:20.159579]: text loss 2.0725 |  3600/ 2358 batches
[2023-05-17 22:33:24.367110]: text loss 2.0678 |  3800/ 2358 batches
[2023-05-17 22:34:29.210913]: text loss 2.0575 |  4000/ 2358 batches
[2023-05-17 22:35:35.106004]: text loss 2.0491 |  4200/ 2358 batches
[2023-05-17 22:36:41.483998]: text loss 2.0762 |  4400/ 2358 batches
[2023-05-17 22:37:48.225408]: text loss 2.0659 |  4600/ 2358 batches
[2023-05-17 22:38:27.138197]: text loss 2.0575 |  4716/ 2358 batches
[2023-05-17 22:38:27.139812]: validation
[2023-05-17 22:38:32.165607]: explanation loss 2.2592
[2023-05-17 22:38:53.407950]: sequential loss 2.3160
[2023-05-17 22:40:55.088280]: top-N loss 1.4684
[2023-05-17 22:40:55.095062]: total loss 2.0145
[2023-05-17 22:40:55.949313]: epoch 3
[2023-05-17 22:41:23.505398]: text loss 2.0380 |  4800/ 2358 batches
[2023-05-17 22:42:19.344789]: text loss 2.0253 |  5000/ 2358 batches
[2023-05-17 22:43:15.493887]: text loss 2.0258 |  5200/ 2358 batches
[2023-05-17 22:44:11.610450]: text loss 2.0298 |  5400/ 2358 batches
[2023-05-17 22:45:07.478958]: text loss 2.0330 |  5600/ 2358 batches
[2023-05-17 22:46:13.828769]: text loss 2.0320 |  5800/ 2358 batches
[2023-05-17 22:47:20.140021]: text loss 2.0233 |  6000/ 2358 batches
[2023-05-17 22:48:25.551618]: text loss 2.0172 |  6200/ 2358 batches
[2023-05-17 22:49:31.723174]: text loss 2.0179 |  6400/ 2358 batches
[2023-05-17 22:50:37.707335]: text loss 2.0080 |  6600/ 2358 batches
[2023-05-17 22:51:43.402935]: text loss 2.0219 |  6800/ 2358 batches
[2023-05-17 22:52:50.091821]: text loss 2.0134 |  7000/ 2358 batches
[2023-05-17 22:53:14.529002]: text loss 1.9964 |  7074/ 2358 batches
[2023-05-17 22:53:14.530537]: validation
[2023-05-17 22:53:19.522551]: explanation loss 2.1976
[2023-05-17 22:53:40.839291]: sequential loss 2.3019
[2023-05-17 22:55:42.321035]: top-N loss 1.4479
[2023-05-17 22:55:42.333314]: total loss 1.9825
[2023-05-17 22:55:43.242927]: epoch 4
[2023-05-17 22:56:24.802909]: text loss 2.0172 |  7200/ 2358 batches
[2023-05-17 22:57:30.412977]: text loss 2.0144 |  7400/ 2358 batches
[2023-05-17 22:58:36.518037]: text loss 1.9890 |  7600/ 2358 batches
[2023-05-17 22:59:42.684418]: text loss 2.0083 |  7800/ 2358 batches
[2023-05-17 23:00:48.485875]: text loss 1.9889 |  8000/ 2358 batches
[2023-05-17 23:01:54.487997]: text loss 1.9932 |  8200/ 2358 batches
[2023-05-17 23:03:01.770414]: text loss 1.9965 |  8400/ 2358 batches
[2023-05-17 23:04:07.505025]: text loss 1.9980 |  8600/ 2358 batches
[2023-05-17 23:05:14.338243]: text loss 1.9855 |  8800/ 2358 batches
[2023-05-17 23:06:21.129199]: text loss 1.9772 |  9000/ 2358 batches
[2023-05-17 23:07:26.972153]: text loss 1.9889 |  9200/ 2358 batches
[2023-05-17 23:08:33.174488]: text loss 1.9988 |  9400/ 2358 batches
[2023-05-17 23:08:43.810951]: text loss 2.0031 |  9432/ 2358 batches
[2023-05-17 23:08:43.812561]: validation
[2023-05-17 23:08:48.675853]: explanation loss 2.1727
[2023-05-17 23:09:09.785937]: sequential loss 2.2909
[2023-05-17 23:11:05.193744]: top-N loss 1.4156
[2023-05-17 23:11:05.194133]: total loss 1.9597
[2023-05-17 23:11:05.987554]: epoch 5
[2023-05-17 23:11:59.526854]: text loss 1.9822 |  9600/ 2358 batches
[2023-05-17 23:13:03.229258]: text loss 1.9821 |  9800/ 2358 batches
[2023-05-17 23:14:07.415946]: text loss 1.9899 | 10000/ 2358 batches
[2023-05-17 23:15:12.379702]: text loss 2.0111 | 10200/ 2358 batches
[2023-05-17 23:16:16.219259]: text loss 2.0096 | 10400/ 2358 batches
[2023-05-17 23:17:21.405096]: text loss 1.9872 | 10600/ 2358 batches
[2023-05-17 23:18:25.826952]: text loss 1.9690 | 10800/ 2358 batches
[2023-05-17 23:19:30.279631]: text loss 1.9745 | 11000/ 2358 batches
[2023-05-17 23:20:35.315232]: text loss 1.9607 | 11200/ 2358 batches
[2023-05-17 23:21:40.218710]: text loss 1.9570 | 11400/ 2358 batches
[2023-05-17 23:22:44.403933]: text loss 1.9707 | 11600/ 2358 batches
[2023-05-17 23:23:46.070434]: text loss 1.9627 | 11790/ 2358 batches
[2023-05-17 23:23:46.071954]: validation
[2023-05-17 23:23:51.155855]: explanation loss 2.1610
[2023-05-17 23:24:11.506550]: sequential loss 2.2769
[2023-05-17 23:26:07.241363]: top-N loss 1.4031
[2023-05-17 23:26:07.241715]: total loss 1.9470
[2023-05-17 23:26:08.074849]: epoch 6
[2023-05-17 23:26:11.117608]: text loss 2.0273 | 11800/ 2358 batches
[2023-05-17 23:27:15.659936]: text loss 1.9441 | 12000/ 2358 batches
[2023-05-17 23:28:20.096244]: text loss 1.9663 | 12200/ 2358 batches
[2023-05-17 23:29:25.509183]: text loss 1.9506 | 12400/ 2358 batches
[2023-05-17 23:30:29.728013]: text loss 1.9512 | 12600/ 2358 batches
[2023-05-17 23:31:33.916051]: text loss 1.9499 | 12800/ 2358 batches
[2023-05-17 23:32:38.542719]: text loss 1.9496 | 13000/ 2358 batches
[2023-05-17 23:33:42.731698]: text loss 1.9386 | 13200/ 2358 batches
[2023-05-17 23:34:46.502577]: text loss 1.9517 | 13400/ 2358 batches
[2023-05-17 23:35:51.647522]: text loss 1.9452 | 13600/ 2358 batches
[2023-05-17 23:36:56.540347]: text loss 1.9547 | 13800/ 2358 batches
[2023-05-17 23:38:00.951467]: text loss 1.9439 | 14000/ 2358 batches
[2023-05-17 23:38:49.271726]: text loss 1.9419 | 14148/ 2358 batches
[2023-05-17 23:38:49.279718]: validation
[2023-05-17 23:38:54.112653]: explanation loss 2.1511
[2023-05-17 23:39:14.706688]: sequential loss 2.2660
[2023-05-17 23:41:10.015730]: top-N loss 1.3845
[2023-05-17 23:41:10.016096]: total loss 1.9339
[2023-05-17 23:41:10.854682]: epoch 7
[2023-05-17 23:41:27.125214]: text loss 1.9444 | 14200/ 2358 batches
[2023-05-17 23:42:31.249420]: text loss 1.9430 | 14400/ 2358 batches
[2023-05-17 23:43:35.488899]: text loss 1.9468 | 14600/ 2358 batches
[2023-05-17 23:44:39.258198]: text loss 1.9343 | 14800/ 2358 batches
[2023-05-17 23:45:43.565501]: text loss 1.9205 | 15000/ 2358 batches
[2023-05-17 23:46:47.337670]: text loss 1.9177 | 15200/ 2358 batches
[2023-05-17 23:47:51.625927]: text loss 1.9166 | 15400/ 2358 batches
[2023-05-17 23:48:56.134521]: text loss 1.9178 | 15600/ 2358 batches
[2023-05-17 23:50:00.036568]: text loss 1.9174 | 15800/ 2358 batches
[2023-05-17 23:51:04.478305]: text loss 1.9147 | 16000/ 2358 batches
[2023-05-17 23:52:09.713549]: text loss 1.9268 | 16200/ 2358 batches
[2023-05-17 23:53:13.834451]: text loss 1.9251 | 16400/ 2358 batches
[2023-05-17 23:53:48.219660]: text loss 1.9325 | 16506/ 2358 batches
[2023-05-17 23:53:48.221135]: validation
[2023-05-17 23:53:53.204303]: explanation loss 2.1423
[2023-05-17 23:54:13.980860]: sequential loss 2.2467
[2023-05-17 23:56:09.413481]: top-N loss 1.3696
[2023-05-17 23:56:09.413843]: total loss 1.9195
[2023-05-17 23:56:10.226312]: epoch 8
[2023-05-17 23:56:40.149887]: text loss 1.9230 | 16600/ 2358 batches
[2023-05-17 23:57:44.513736]: text loss 1.9266 | 16800/ 2358 batches
[2023-05-17 23:58:48.115396]: text loss 1.9208 | 17000/ 2358 batches
[2023-05-17 23:59:53.549661]: text loss 1.9108 | 17200/ 2358 batches
[2023-05-18 00:00:59.780547]: text loss 1.9213 | 17400/ 2358 batches
[2023-05-18 00:02:05.594270]: text loss 1.9198 | 17600/ 2358 batches
[2023-05-18 00:03:12.573861]: text loss 1.9027 | 17800/ 2358 batches
[2023-05-18 00:04:19.673520]: text loss 1.9092 | 18000/ 2358 batches
[2023-05-18 00:05:26.130210]: text loss 1.9036 | 18200/ 2358 batches
[2023-05-18 00:06:32.651578]: text loss 1.9044 | 18400/ 2358 batches
[2023-05-18 00:07:39.405044]: text loss 1.9094 | 18600/ 2358 batches
[2023-05-18 00:08:45.795199]: text loss 1.9068 | 18800/ 2358 batches
[2023-05-18 00:09:07.743382]: text loss 1.8996 | 18864/ 2358 batches
[2023-05-18 00:09:07.744724]: validation
[2023-05-18 00:09:12.659200]: explanation loss 2.1337
[2023-05-18 00:09:33.853614]: sequential loss 2.2332
[2023-05-18 00:11:30.257173]: top-N loss 1.3481
[2023-05-18 00:11:30.257543]: total loss 1.9050
[2023-05-18 00:11:31.084857]: epoch 9
[2023-05-18 00:12:14.772194]: text loss 1.8992 | 19000/ 2358 batches
[2023-05-18 00:13:20.926223]: text loss 1.8872 | 19200/ 2358 batches
[2023-05-18 00:14:27.228135]: text loss 1.8927 | 19400/ 2358 batches
[2023-05-18 00:15:33.826125]: text loss 1.8968 | 19600/ 2358 batches
[2023-05-18 00:16:40.505568]: text loss 1.8956 | 19800/ 2358 batches
[2023-05-18 00:17:46.612895]: text loss 1.8943 | 20000/ 2358 batches
[2023-05-18 00:18:53.479375]: text loss 1.8657 | 20200/ 2358 batches
[2023-05-18 00:19:59.454936]: text loss 1.8731 | 20400/ 2358 batches
[2023-05-18 00:21:05.125001]: text loss 1.8825 | 20600/ 2358 batches
[2023-05-18 00:22:11.131597]: text loss 1.8775 | 20800/ 2358 batches
[2023-05-18 00:23:17.525750]: text loss 1.8779 | 21000/ 2358 batches
[2023-05-18 00:24:23.699263]: text loss 1.8918 | 21200/ 2358 batches
[2023-05-18 00:24:31.381390]: text loss 1.8615 | 21222/ 2358 batches
[2023-05-18 00:24:31.383043]: validation
[2023-05-18 00:24:36.450492]: explanation loss 2.1291
[2023-05-18 00:24:57.618833]: sequential loss 2.2242
[2023-05-18 00:26:54.964671]: top-N loss 1.3375
[2023-05-18 00:26:54.965081]: total loss 1.8969
[2023-05-18 00:26:55.725454]: epoch 10
[2023-05-18 00:27:53.483983]: text loss 1.8828 | 21400/ 2358 batches
[2023-05-18 00:28:59.517096]: text loss 1.8766 | 21600/ 2358 batches
[2023-05-18 00:30:04.379979]: text loss 1.8685 | 21800/ 2358 batches
[2023-05-18 00:31:09.088681]: text loss 1.8613 | 22000/ 2358 batches
[2023-05-18 00:32:13.310241]: text loss 1.8761 | 22200/ 2358 batches
[2023-05-18 00:33:17.606568]: text loss 1.8747 | 22400/ 2358 batches
[2023-05-18 00:34:23.089073]: text loss 1.8650 | 22600/ 2358 batches
[2023-05-18 00:35:27.737845]: text loss 1.8785 | 22800/ 2358 batches
[2023-05-18 00:36:31.559345]: text loss 1.8810 | 23000/ 2358 batches
[2023-05-18 00:37:36.382172]: text loss 1.8622 | 23200/ 2358 batches
[2023-05-18 00:38:41.722574]: text loss 1.8623 | 23400/ 2358 batches
[2023-05-18 00:39:39.907133]: text loss 1.8676 | 23580/ 2358 batches
[2023-05-18 00:39:39.908692]: validation
[2023-05-18 00:39:44.996995]: explanation loss 2.1210
[2023-05-18 00:40:05.599228]: sequential loss 2.2056
[2023-05-18 00:42:01.362039]: top-N loss 1.3198
[2023-05-18 00:42:01.362390]: total loss 1.8821
[2023-05-18 00:42:02.184688]: epoch 11
[2023-05-18 00:42:08.171094]: text loss 1.8826 | 23600/ 2358 batches
[2023-05-18 00:43:13.129485]: text loss 1.8764 | 23800/ 2358 batches
[2023-05-18 00:44:18.789223]: text loss 1.8613 | 24000/ 2358 batches
[2023-05-18 00:45:22.703128]: text loss 1.8665 | 24200/ 2358 batches
[2023-05-18 00:46:27.655909]: text loss 1.8583 | 24400/ 2358 batches
[2023-05-18 00:47:32.577701]: text loss 1.8634 | 24600/ 2358 batches
[2023-05-18 00:48:36.319130]: text loss 1.8639 | 24800/ 2358 batches
[2023-05-18 00:49:41.301630]: text loss 1.8522 | 25000/ 2358 batches
[2023-05-18 00:50:45.839515]: text loss 1.8570 | 25200/ 2358 batches
[2023-05-18 00:51:50.114536]: text loss 1.8549 | 25400/ 2358 batches
[2023-05-18 00:52:55.417283]: text loss 1.8407 | 25600/ 2358 batches
[2023-05-18 00:54:01.243816]: text loss 1.8368 | 25800/ 2358 batches
[2023-05-18 00:54:45.371196]: text loss 1.8581 | 25938/ 2358 batches
[2023-05-18 00:54:45.374002]: validation
[2023-05-18 00:54:50.332105]: explanation loss 2.1248
[2023-05-18 00:55:11.114168]: sequential loss 2.1893
[2023-05-18 00:57:05.796460]: top-N loss 1.3092
[2023-05-18 00:57:05.796737]: total loss 1.8744
[2023-05-18 00:57:06.603160]: epoch 12
[2023-05-18 00:57:26.049014]: text loss 1.8708 | 26000/ 2358 batches
[2023-05-18 00:58:30.429768]: text loss 1.8376 | 26200/ 2358 batches
[2023-05-18 00:59:35.210538]: text loss 1.8468 | 26400/ 2358 batches
[2023-05-18 01:00:39.082568]: text loss 1.8443 | 26600/ 2358 batches
[2023-05-18 01:01:44.114434]: text loss 1.8301 | 26800/ 2358 batches
[2023-05-18 01:02:49.158720]: text loss 1.8339 | 27000/ 2358 batches
[2023-05-18 01:03:53.956819]: text loss 1.8492 | 27200/ 2358 batches
[2023-05-18 01:05:00.505297]: text loss 1.8389 | 27400/ 2358 batches
[2023-05-18 01:06:06.948445]: text loss 1.8346 | 27600/ 2358 batches
[2023-05-18 01:07:11.906556]: text loss 1.8345 | 27800/ 2358 batches
[2023-05-18 01:08:17.826570]: text loss 1.8310 | 28000/ 2358 batches
[2023-05-18 01:09:23.575397]: text loss 1.8421 | 28200/ 2358 batches
[2023-05-18 01:09:54.319224]: text loss 1.8239 | 28296/ 2358 batches
[2023-05-18 01:09:54.320549]: validation
[2023-05-18 01:09:59.275003]: explanation loss 2.1176
[2023-05-18 01:10:19.825196]: sequential loss 2.1870
[2023-05-18 01:12:09.342643]: top-N loss 1.3032
[2023-05-18 01:12:09.342908]: total loss 1.8693
[2023-05-18 01:12:10.151738]: epoch 13
[2023-05-18 01:12:39.056853]: text loss 1.8353 | 28400/ 2358 batches
[2023-05-18 01:13:35.732477]: text loss 1.8301 | 28600/ 2358 batches
[2023-05-18 01:14:31.724135]: text loss 1.8352 | 28800/ 2358 batches
[2023-05-18 01:15:26.894380]: text loss 1.8385 | 29000/ 2358 batches
[2023-05-18 01:16:22.632432]: text loss 1.8336 | 29200/ 2358 batches
[2023-05-18 01:17:18.565282]: text loss 1.8283 | 29400/ 2358 batches
[2023-05-18 01:18:13.749422]: text loss 1.8174 | 29600/ 2358 batches
[2023-05-18 01:19:09.867355]: text loss 1.8316 | 29800/ 2358 batches
[2023-05-18 01:20:05.727394]: text loss 1.8266 | 30000/ 2358 batches
[2023-05-18 01:21:01.164612]: text loss 1.8156 | 30200/ 2358 batches
[2023-05-18 01:21:57.017738]: text loss 1.8000 | 30400/ 2358 batches
[2023-05-18 01:22:52.924041]: text loss 1.8171 | 30600/ 2358 batches
[2023-05-18 01:23:07.905215]: text loss 1.8224 | 30654/ 2358 batches
[2023-05-18 01:23:07.906168]: validation
[2023-05-18 01:23:12.186077]: explanation loss 2.1179
[2023-05-18 01:23:29.783572]: sequential loss 2.1783
[2023-05-18 01:25:10.721682]: top-N loss 1.2912
[2023-05-18 01:25:10.722028]: total loss 1.8624
[2023-05-18 01:25:11.614520]: epoch 14
[2023-05-18 01:25:52.036509]: text loss 1.8179 | 30800/ 2358 batches
[2023-05-18 01:26:47.926271]: text loss 1.8106 | 31000/ 2358 batches
[2023-05-18 01:27:43.804937]: text loss 1.8125 | 31200/ 2358 batches
[2023-05-18 01:28:39.544206]: text loss 1.8085 | 31400/ 2358 batches
[2023-05-18 01:29:35.640849]: text loss 1.7948 | 31600/ 2358 batches
[2023-05-18 01:30:31.647705]: text loss 1.8131 | 31800/ 2358 batches
[2023-05-18 01:31:27.056144]: text loss 1.8079 | 32000/ 2358 batches
[2023-05-18 01:32:22.980672]: text loss 1.8061 | 32200/ 2358 batches
[2023-05-18 01:33:18.839911]: text loss 1.8064 | 32400/ 2358 batches
[2023-05-18 01:34:14.241794]: text loss 1.8131 | 32600/ 2358 batches
[2023-05-18 01:35:09.908613]: text loss 1.8090 | 32800/ 2358 batches
[2023-05-18 01:36:05.866049]: text loss 1.8159 | 33000/ 2358 batches
[2023-05-18 01:36:09.173655]: text loss 1.7985 | 33012/ 2358 batches
[2023-05-18 01:36:09.174513]: validation
[2023-05-18 01:36:13.487616]: explanation loss 2.1120
[2023-05-18 01:36:31.127571]: sequential loss 2.1731
[2023-05-18 01:38:11.867921]: top-N loss 1.2880
[2023-05-18 01:38:11.868252]: total loss 1.8577
[2023-05-18 01:38:12.644613]: epoch 15
[2023-05-18 01:39:04.503639]: text loss 1.8173 | 33200/ 2358 batches
[2023-05-18 01:40:00.348123]: text loss 1.8060 | 33400/ 2358 batches
[2023-05-18 01:40:56.252636]: text loss 1.8073 | 33600/ 2358 batches
[2023-05-18 01:41:51.776598]: text loss 1.8074 | 33800/ 2358 batches
[2023-05-18 01:42:47.618220]: text loss 1.8079 | 34000/ 2358 batches
[2023-05-18 01:43:43.434931]: text loss 1.8068 | 34200/ 2358 batches
[2023-05-18 01:44:39.018212]: text loss 1.8004 | 34400/ 2358 batches
[2023-05-18 01:45:35.216926]: text loss 1.8030 | 34600/ 2358 batches
[2023-05-18 01:46:31.264172]: text loss 1.7805 | 34800/ 2358 batches
[2023-05-18 01:47:27.104351]: text loss 1.7976 | 35000/ 2358 batches
[2023-05-18 01:48:23.549386]: text loss 1.7763 | 35200/ 2358 batches
[2023-05-18 01:49:11.648266]: text loss 1.7903 | 35370/ 2358 batches
[2023-05-18 01:49:11.649460]: validation
[2023-05-18 01:49:15.942809]: explanation loss 2.1124
[2023-05-18 01:49:33.628703]: sequential loss 2.1741
[2023-05-18 01:51:14.818411]: top-N loss 1.2798
[2023-05-18 01:51:14.818648]: total loss 1.8554
[2023-05-18 01:51:15.610850]: epoch 16
[2023-05-18 01:51:23.917909]: text loss 1.8131 | 35400/ 2358 batches
[2023-05-18 01:52:19.182531]: text loss 1.7837 | 35600/ 2358 batches
[2023-05-18 01:53:15.316043]: text loss 1.7859 | 35800/ 2358 batches
[2023-05-18 01:54:11.373351]: text loss 1.7756 | 36000/ 2358 batches
[2023-05-18 01:55:06.857214]: text loss 1.7765 | 36200/ 2358 batches
[2023-05-18 01:56:02.650738]: text loss 1.7771 | 36400/ 2358 batches
[2023-05-18 01:56:58.615583]: text loss 1.7793 | 36600/ 2358 batches
[2023-05-18 01:57:54.218411]: text loss 1.7845 | 36800/ 2358 batches
[2023-05-18 01:58:50.189851]: text loss 1.7835 | 37000/ 2358 batches
[2023-05-18 01:59:46.295191]: text loss 1.7728 | 37200/ 2358 batches
[2023-05-18 02:00:41.594731]: text loss 1.7791 | 37400/ 2358 batches
[2023-05-18 02:01:37.581885]: text loss 1.7754 | 37600/ 2358 batches
[2023-05-18 02:02:14.375676]: text loss 1.7817 | 37728/ 2358 batches
[2023-05-18 02:02:14.376778]: validation
[2023-05-18 02:02:18.813743]: explanation loss 2.1099
[2023-05-18 02:02:37.215696]: sequential loss 2.1662
[2023-05-18 02:04:18.906794]: top-N loss 1.2630
[2023-05-18 02:04:18.907113]: total loss 1.8464
[2023-05-18 02:04:19.693778]: epoch 17
[2023-05-18 02:04:39.675546]: text loss 1.7839 | 37800/ 2358 batches
[2023-05-18 02:05:35.094467]: text loss 1.7759 | 38000/ 2358 batches
[2023-05-18 02:06:31.162306]: text loss 1.7689 | 38200/ 2358 batches
[2023-05-18 02:07:27.160552]: text loss 1.7662 | 38400/ 2358 batches
[2023-05-18 02:08:22.541102]: text loss 1.7909 | 38600/ 2358 batches
[2023-05-18 02:09:18.187649]: text loss 1.7817 | 38800/ 2358 batches
[2023-05-18 02:10:13.838630]: text loss 1.7742 | 39000/ 2358 batches
[2023-05-18 02:11:09.432302]: text loss 1.7766 | 39200/ 2358 batches
[2023-05-18 02:12:05.199183]: text loss 1.7763 | 39400/ 2358 batches
[2023-05-18 02:13:01.231303]: text loss 1.7760 | 39600/ 2358 batches
[2023-05-18 02:13:56.401451]: text loss 1.7687 | 39800/ 2358 batches
[2023-05-18 02:14:52.108525]: text loss 1.7546 | 40000/ 2358 batches
[2023-05-18 02:15:16.125939]: text loss 1.7585 | 40086/ 2358 batches
[2023-05-18 02:15:16.127195]: validation
[2023-05-18 02:15:20.402183]: explanation loss 2.1097
[2023-05-18 02:15:37.897495]: sequential loss 2.1688
[2023-05-18 02:17:18.181883]: top-N loss 1.2611
[2023-05-18 02:17:18.182262]: total loss 1.8465
[2023-05-18 02:17:18.182290]: Endured 1 time(s)
[2023-05-18 02:17:18.182324]: epoch 18
[2023-05-18 02:17:49.892346]: text loss 1.7481 | 40200/ 2358 batches
[2023-05-18 02:18:45.242478]: text loss 1.7592 | 40400/ 2358 batches
[2023-05-18 02:19:40.793313]: text loss 1.7551 | 40600/ 2358 batches
[2023-05-18 02:20:36.653874]: text loss 1.7517 | 40800/ 2358 batches
[2023-05-18 02:21:32.059974]: text loss 1.7561 | 41000/ 2358 batches
[2023-05-18 02:22:27.674703]: text loss 1.7538 | 41200/ 2358 batches
[2023-05-18 02:23:23.479216]: text loss 1.7666 | 41400/ 2358 batches
[2023-05-18 02:24:18.738984]: text loss 1.7554 | 41600/ 2358 batches
[2023-05-18 02:25:14.684579]: text loss 1.7525 | 41800/ 2358 batches
[2023-05-18 02:26:10.379301]: text loss 1.7467 | 42000/ 2358 batches
[2023-05-18 02:27:05.584240]: text loss 1.7490 | 42200/ 2358 batches
[2023-05-18 02:28:01.273019]: text loss 1.7529 | 42400/ 2358 batches
[2023-05-18 02:28:13.507110]: text loss 1.7373 | 42444/ 2358 batches
[2023-05-18 02:28:13.507721]: validation
[2023-05-18 02:28:17.794893]: explanation loss 2.1094
[2023-05-18 02:28:35.252208]: sequential loss 2.1631
[2023-05-18 02:30:15.463686]: top-N loss 1.2525
[2023-05-18 02:30:15.464129]: total loss 1.8417
[2023-05-18 02:30:16.238950]: epoch 19
[2023-05-18 02:30:59.688035]: text loss 1.7541 | 42600/ 2358 batches
[2023-05-18 02:31:54.852249]: text loss 1.7547 | 42800/ 2358 batches
[2023-05-18 02:32:51.121404]: text loss 1.7579 | 43000/ 2358 batches
[2023-05-18 02:33:47.473547]: text loss 1.7460 | 43200/ 2358 batches
[2023-05-18 02:34:42.124903]: text loss 1.7493 | 43400/ 2358 batches
[2023-05-18 02:35:38.092169]: text loss 1.7443 | 43600/ 2358 batches
[2023-05-18 02:36:33.636140]: text loss 1.7409 | 43800/ 2358 batches
[2023-05-18 02:37:29.076005]: text loss 1.7599 | 44000/ 2358 batches
[2023-05-18 02:38:24.439060]: text loss 1.7328 | 44200/ 2358 batches
[2023-05-18 02:39:21.026044]: text loss 1.7405 | 44400/ 2358 batches
[2023-05-18 02:40:16.613358]: text loss 1.7406 | 44600/ 2358 batches
[2023-05-18 02:41:12.924179]: text loss 1.7432 | 44800/ 2358 batches
[2023-05-18 02:41:13.663568]: text loss 1.5239 | 44802/ 2358 batches
[2023-05-18 02:41:13.664236]: validation
[2023-05-18 02:41:18.087315]: explanation loss 2.1015
[2023-05-18 02:41:36.274747]: sequential loss 2.1612
[2023-05-18 02:43:16.919147]: top-N loss 1.2494
[2023-05-18 02:43:16.919535]: total loss 1.8374
[2023-05-18 02:43:17.687846]: epoch 20
[2023-05-18 02:44:12.639206]: text loss 1.7481 | 45000/ 2358 batches
[2023-05-18 02:45:07.571937]: text loss 1.7140 | 45200/ 2358 batches
[2023-05-18 02:46:03.184112]: text loss 1.7336 | 45400/ 2358 batches
[2023-05-18 02:46:58.914164]: text loss 1.7227 | 45600/ 2358 batches
[2023-05-18 02:47:54.352093]: text loss 1.7278 | 45800/ 2358 batches
[2023-05-18 02:48:49.982085]: text loss 1.7105 | 46000/ 2358 batches
[2023-05-18 02:49:45.546057]: text loss 1.7291 | 46200/ 2358 batches
[2023-05-18 02:50:40.371135]: text loss 1.7343 | 46400/ 2358 batches
[2023-05-18 02:51:35.903777]: text loss 1.7227 | 46600/ 2358 batches
[2023-05-18 02:52:31.335876]: text loss 1.7263 | 46800/ 2358 batches
[2023-05-18 02:53:26.699235]: text loss 1.7408 | 47000/ 2358 batches
[2023-05-18 02:54:12.574078]: text loss 1.7252 | 47160/ 2358 batches
[2023-05-18 02:54:12.575380]: validation
[2023-05-18 02:54:16.900789]: explanation loss 2.1050
[2023-05-18 02:54:34.589132]: sequential loss 2.1588
[2023-05-18 02:56:16.972059]: top-N loss 1.2349
[2023-05-18 02:56:16.972328]: total loss 1.8329
[2023-05-18 02:56:17.751363]: epoch 21
[2023-05-18 02:56:29.281718]: text loss 1.7263 | 47200/ 2358 batches
[2023-05-18 02:57:26.676003]: text loss 1.7118 | 47400/ 2358 batches
[2023-05-18 02:58:23.592575]: text loss 1.7311 | 47600/ 2358 batches
[2023-05-18 02:59:20.059045]: text loss 1.7299 | 47800/ 2358 batches
[2023-05-18 03:00:16.794738]: text loss 1.7279 | 48000/ 2358 batches
[2023-05-18 03:01:12.498743]: text loss 1.7286 | 48200/ 2358 batches
[2023-05-18 03:02:08.707448]: text loss 1.7190 | 48400/ 2358 batches
[2023-05-18 03:03:07.162359]: text loss 1.7061 | 48600/ 2358 batches
[2023-05-18 03:04:04.484450]: text loss 1.7271 | 48800/ 2358 batches
[2023-05-18 03:05:02.066588]: text loss 1.7112 | 49000/ 2358 batches
[2023-05-18 03:05:59.237627]: text loss 1.7115 | 49200/ 2358 batches
[2023-05-18 03:06:57.550832]: text loss 1.7158 | 49400/ 2358 batches
[2023-05-18 03:07:32.227510]: text loss 1.7230 | 49518/ 2358 batches
[2023-05-18 03:07:32.228980]: validation
[2023-05-18 03:07:36.553358]: explanation loss 2.0981
[2023-05-18 03:07:55.137504]: sequential loss 2.1722
[2023-05-18 03:09:38.069264]: top-N loss 1.2331
[2023-05-18 03:09:38.069633]: total loss 1.8344
[2023-05-18 03:09:38.069655]: Endured 2 time(s)
[2023-05-18 03:09:38.069669]: epoch 22
[2023-05-18 03:10:01.136263]: text loss 1.7135 | 49600/ 2358 batches
[2023-05-18 03:10:58.039591]: text loss 1.7164 | 49800/ 2358 batches
[2023-05-18 03:11:55.631599]: text loss 1.7109 | 50000/ 2358 batches
[2023-05-18 03:12:51.435236]: text loss 1.6969 | 50200/ 2358 batches
[2023-05-18 03:13:47.153171]: text loss 1.6956 | 50400/ 2358 batches
[2023-05-18 03:14:42.219192]: text loss 1.7062 | 50600/ 2358 batches
[2023-05-18 03:15:38.048967]: text loss 1.6879 | 50800/ 2358 batches
[2023-05-18 03:16:35.341543]: text loss 1.6941 | 51000/ 2358 batches
[2023-05-18 03:17:32.699815]: text loss 1.7098 | 51200/ 2358 batches
[2023-05-18 03:18:30.224820]: text loss 1.7024 | 51400/ 2358 batches
[2023-05-18 03:19:27.768684]: text loss 1.6924 | 51600/ 2358 batches
[2023-05-18 03:20:25.039740]: text loss 1.6976 | 51800/ 2358 batches
[2023-05-18 03:20:47.141359]: text loss 1.6755 | 51876/ 2358 batches
[2023-05-18 03:20:47.142605]: validation
[2023-05-18 03:20:51.453815]: explanation loss 2.1053
[2023-05-18 03:21:09.499237]: sequential loss 2.1771
[2023-05-18 03:22:53.929779]: top-N loss 1.2362
[2023-05-18 03:22:53.930169]: total loss 1.8395
[2023-05-18 03:22:53.930188]: Endured 3 time(s)
[2023-05-18 03:22:53.930200]: epoch 23
[2023-05-18 03:23:28.822970]: text loss 1.6981 | 52000/ 2358 batches
[2023-05-18 03:24:25.235290]: text loss 1.6940 | 52200/ 2358 batches
[2023-05-18 03:25:21.332000]: text loss 1.6988 | 52400/ 2358 batches
[2023-05-18 03:26:18.388884]: text loss 1.6892 | 52600/ 2358 batches
[2023-05-18 03:27:15.030879]: text loss 1.6795 | 52800/ 2358 batches
[2023-05-18 03:28:11.503565]: text loss 1.6957 | 53000/ 2358 batches
[2023-05-18 03:29:08.605030]: text loss 1.6876 | 53200/ 2358 batches
[2023-05-18 03:30:05.303739]: text loss 1.6889 | 53400/ 2358 batches
[2023-05-18 03:31:01.904220]: text loss 1.6895 | 53600/ 2358 batches
[2023-05-18 03:31:58.768349]: text loss 1.6983 | 53800/ 2358 batches
[2023-05-18 03:32:55.646495]: text loss 1.6933 | 54000/ 2358 batches
[2023-05-18 03:33:51.536366]: text loss 1.6917 | 54200/ 2358 batches
[2023-05-18 03:34:01.475757]: text loss 1.6806 | 54234/ 2358 batches
[2023-05-18 03:34:01.477104]: validation
[2023-05-18 03:34:05.790798]: explanation loss 2.0998
[2023-05-18 03:34:23.581485]: sequential loss 2.1798
[2023-05-18 03:36:07.860753]: top-N loss 1.2227
[2023-05-18 03:36:07.861212]: total loss 1.8341
[2023-05-18 03:36:07.861233]: Endured 4 time(s)
[2023-05-18 03:36:07.861247]: epoch 24
[2023-05-18 03:36:54.654633]: text loss 1.7035 | 54400/ 2358 batches
[2023-05-18 03:37:51.357271]: text loss 1.6813 | 54600/ 2358 batches
[2023-05-18 03:38:48.027016]: text loss 1.6924 | 54800/ 2358 batches
[2023-05-18 03:39:44.777044]: text loss 1.6742 | 55000/ 2358 batches
[2023-05-18 03:40:41.480701]: text loss 1.6694 | 55200/ 2358 batches
[2023-05-18 03:41:37.966168]: text loss 1.6734 | 55400/ 2358 batches
[2023-05-18 03:42:34.837188]: text loss 1.6673 | 55600/ 2358 batches
[2023-05-18 03:43:31.458084]: text loss 1.6641 | 55800/ 2358 batches
[2023-05-18 03:44:27.592230]: text loss 1.6727 | 56000/ 2358 batches
[2023-05-18 03:45:24.786022]: text loss 1.6677 | 56200/ 2358 batches
[2023-05-18 03:46:21.631851]: text loss 1.6666 | 56400/ 2358 batches
[2023-05-18 03:47:16.432148]: text loss 1.6771 | 56592/ 2358 batches
[2023-05-18 03:47:16.433609]: validation
[2023-05-18 03:47:20.865588]: explanation loss 2.1047
[2023-05-18 03:47:38.983049]: sequential loss 2.1738
[2023-05-18 03:49:22.564229]: top-N loss 1.2270
[2023-05-18 03:49:22.564500]: total loss 1.8352
[2023-05-18 03:49:22.564519]: Endured 5 time(s)
[2023-05-18 03:49:22.564531]: Cannot endure it anymore | Exiting from early stop
----------------------------------------ARGUMENTS----------------------------------------
data_dir                                 ./data/beauty/
model_version                            0
batch_size                               32
cuda                                     True
checkpoint                               ./checkpoint/beauty/
seq_len                                  50
num_beams                                20
top_n                                    10
----------------------------------------ARGUMENTS----------------------------------------
[2023-05-18 03:58:49.184551]: Loading data
[2023-05-18 03:59:15.771993]: Generating recommendations
[2023-05-18 04:13:13.631338]: HR@1  0.0235
[2023-05-18 04:13:13.643774]: NDCG@1  0.0235
[2023-05-18 04:13:13.654945]: HR@5  0.0537
[2023-05-18 04:13:13.670486]: NDCG@5  0.0395
[2023-05-18 04:13:13.684586]: HR@10  0.0688
[2023-05-18 04:13:13.704116]: NDCG@10  0.0443
----------------------------------------ARGUMENTS----------------------------------------
data_dir                                 ./data/beauty/
model_version                            0
batch_size                               32
cuda                                     True
checkpoint                               ./checkpoint/beauty/
negative_num                             99
seq_len                                  50
num_beams                                20
top_n                                    10
----------------------------------------ARGUMENTS----------------------------------------
[2023-05-18 04:13:15.239092]: Loading data
[2023-05-18 04:13:42.115010]: Generating recommendations
[2023-05-18 04:43:17.994987]: HR@1  0.0829
[2023-05-18 04:43:18.008122]: NDCG@1  0.0829
[2023-05-18 04:43:18.019640]: HR@5  0.1926
[2023-05-18 04:43:18.036508]: NDCG@5  0.1391
[2023-05-18 04:43:18.050915]: HR@10  0.2670
[2023-05-18 04:43:18.072044]: NDCG@10  0.1629
----------------------------------------ARGUMENTS----------------------------------------
data_dir                                 ./data/beauty/
model_version                            0
batch_size                               32
cuda                                     True
checkpoint                               ./checkpoint/beauty/
outf                                     generated.txt
exp_len                                  20
----------------------------------------ARGUMENTS----------------------------------------
[2023-05-21 01:00:19.125749]: Loading data
[2023-05-21 01:00:27.259850]: Generating text
[2023-05-21 01:11:46.083309]: BLEU-1 14.7802
[2023-05-21 01:11:47.204238]: BLEU-4  1.0630
[2023-05-21 01:11:48.339857]: rouge_1/f_score 15.2517
[2023-05-21 01:11:48.339932]: rouge_1/r_score 17.6114
[2023-05-21 01:11:48.339942]: rouge_1/p_score 14.8436
[2023-05-21 01:11:48.339950]: rouge_2/f_score  1.5737
[2023-05-21 01:11:48.339957]: rouge_2/r_score  2.1259
[2023-05-21 01:11:48.339964]: rouge_2/p_score  1.4608
[2023-05-21 01:11:48.339971]: rouge_l/f_score 11.3283
[2023-05-21 01:11:48.339978]: rouge_l/r_score 14.4220
[2023-05-21 01:11:48.339985]: rouge_l/p_score 12.0260
[2023-05-21 01:11:48.351120]: Generated text saved to (./checkpoint/beauty/generated.txt)
